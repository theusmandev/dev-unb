{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMEexRHHYLxKpzjgoj3aT9r"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":342},"id":"EHxvIZR724XS","executionInfo":{"status":"error","timestamp":1766488242739,"user_tz":-300,"elapsed":11258,"user":{"displayName":"Devurdunovelbank","userId":"02511973306613964362"}},"outputId":"79c57585-f906-47e0-9183-3250bd1eba19"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Fetching from: https://www.urdunovelbanks.com/2025/12/\n","Found 17 unique posts.\n","1/17 scraped\n","2/17 scraped\n","3/17 scraped\n","4/17 scraped\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1398348451.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mnovels_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{idx}/{len(post_links)} scraped\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnovels_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import requests\n","from bs4 import BeautifulSoup\n","import pandas as pd\n","import time\n","import os\n","import random\n","from datetime import datetime\n","from google.colab import drive\n","\n","# Mount Drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","SAVE_PATH_XLSX = \"/content/drive/My Drive/UrduNovelBank_Latest.xlsx\"\n","FAILED_LINKS_FILE = \"/content/drive/My Drive/failed_links.txt\"\n","\n","HEADERS = {\n","    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36\"\n","}\n","session = requests.Session()\n","session.headers.update(HEADERS)\n","\n","today = datetime.today()\n","YEAR = today.year\n","MONTH = today.month\n","BASE_URL = f\"https://www.urdunovelbanks.com/{YEAR}/{MONTH:02d}/\"\n","\n","print(f\"Fetching from: {BASE_URL}\")\n","\n","def get_all_post_links(base_url):\n","    post_links = set()  # Use set to avoid duplicates\n","    next_page = base_url\n","\n","    while next_page:\n","        response = session.get(next_page, timeout=30)\n","        if response.status_code != 200:\n","            print(f\"Failed to fetch {next_page}\")\n","            break\n","\n","        soup = BeautifulSoup(response.text, \"html.parser\")\n","\n","        # Better: target actual post title links\n","        for a_tag in soup.select(\".post-title a, h3 a, .entry-title a\"):\n","            href = a_tag.get(\"href\")\n","            if href and href not in post_links:\n","                post_links.add(href)\n","\n","        # Next page\n","        next_link = soup.find(\"a\", string=\"Older Posts\")\n","        if next_link and next_link.get(\"href\"):\n","            next_url = next_link[\"href\"]\n","            if f\"/{YEAR}/{MONTH:02d}/\" in next_url:\n","                next_page = next_url\n","                time.sleep(random.uniform(1, 3))\n","            else:\n","                next_page = None\n","        else:\n","            next_page = None\n","\n","    return list(post_links)\n","\n","post_links = get_all_post_links(BASE_URL)\n","print(f\"Found {len(post_links)} unique posts.\")\n","\n","def scrape_post(post_url):\n","    for attempt in range(3):\n","        try:\n","            response = session.get(post_url, timeout=30)\n","            if response.status_code != 200:\n","                time.sleep(3)\n","                continue\n","\n","            soup = BeautifulSoup(response.text, \"html.parser\")\n","\n","            # Better title\n","            title_tag = soup.find(\"h3\", class_=\"post-title\") or soup.find(\"h1\")\n","            title = title_tag.get_text(strip=True) if title_tag else \"No Title\"\n","\n","            all_links = [a[\"href\"] for a in soup.find_all(\"a\", href=True)]\n","            gdrive = [l for l in all_links if \"drive.google.com\" in l or \"googleusercontent.com\" in l]\n","            mediafire = [l for l in all_links if \"mediafire.com\" in l]\n","\n","            return {\n","                \"Title\": title,\n","                \"URL\": post_url,\n","                \"Google Drive Links\": \", \".join(gdrive) if gdrive else \"None\",\n","                \"Mediafire Links\": \", \".join(mediafire) if mediafire else \"None\",\n","                \"Scraped Date\": today.strftime(\"%Y-%m-%d\")\n","            }\n","        except Exception as e:\n","            print(f\"Attempt {attempt+1} failed for {post_url}: {e}\")\n","            time.sleep(5)\n","\n","    with open(FAILED_LINKS_FILE, \"a\") as f:\n","        f.write(post_url + \"\\n\")\n","    return None\n","\n","novels_data = []\n","for idx, link in enumerate(post_links, 1):\n","    data = scrape_post(link)\n","    if data:\n","        novels_data.append(data)\n","    print(f\"{idx}/{len(post_links)} scraped\")\n","    time.sleep(random.uniform(1.5, 3))\n","\n","if novels_data:\n","    df_new = pd.DataFrame(novels_data)\n","\n","    if os.path.exists(SAVE_PATH_XLSX):\n","        df_old = pd.read_excel(SAVE_PATH_XLSX)\n","        df = pd.concat([df_old, df_new], ignore_index=True)\n","    else:\n","        df = df_new\n","\n","    df = df.drop_duplicates(subset=[\"URL\"])  # Critical!\n","    df.to_excel(SAVE_PATH_XLSX, index=False, engine=\"openpyxl\")\n","    print(f\"Saved {len(df)} total entries to Excel.\")\n","else:\n","    print(\"No new data scraped.\")"]}]}